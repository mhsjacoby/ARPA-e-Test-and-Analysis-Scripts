{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import ast\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeData():\n",
    "    def __init__(self, path):\n",
    "        self.root_dir = path\n",
    "        self.write_dir = self.make_storage_directory(os.path.join(self.root_dir, 'Summaries'))\n",
    "        self.home = path.split('/')[-1].split('-')[-2]\n",
    "        self.system = path.split('/')[-1].split('-')[-1]\n",
    "        self.average_length = 1\n",
    "    \n",
    "    def mylistdir(self, directory):\n",
    "        filelist = os.listdir(directory)\n",
    "        return [x for x in filelist if not (x.startswith('.') or 'Icon' in x)] \n",
    "\n",
    "    def make_storage_directory(self, target_dir):\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "        return target_dir\n",
    "    \n",
    "    def date_segments(self, dates):\n",
    "        output = []\n",
    "        cur_list = [dates[0]]\n",
    "        for dt_pair in zip(dates[1:], dates):\n",
    "            if (dt_pair[0] - dt_pair[1]).days > 1:\n",
    "                output.append(cur_list)\n",
    "                cur_list = [dt_pair[0]]\n",
    "            else:\n",
    "                cur_list.append(dt_pair[0])\n",
    "        output.append(cur_list)\n",
    "        return output   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeOccupancy(HomeData):\n",
    "    \n",
    "    def __init__(self, path, freq = '10S'):      \n",
    "        HomeData.__init__(self, path) \n",
    "        self.ground_path = os.path.join(self.root_dir, 'GroundTruth')\n",
    "        self.occ_freq = freq    \n",
    "        self.occupant_names = []\n",
    "\n",
    "    def mylistdir(self, directory):\n",
    "        filelist = os.listdir(directory)\n",
    "        return [x for x in filelist if x.endswith('.csv')]        \n",
    "        \n",
    "    def get_ground_truth(self):\n",
    "        occupant_files = self.mylistdir(self.ground_path)\n",
    "        occupants = {}\n",
    "        enter_times, exit_times = [], []\n",
    "        \n",
    "        for occ in occupant_files:\n",
    "            #occupant_name = occ.strip('.csv').split('-')[1] ## H3-black\n",
    "            occupant_name = occ.strip('.csv').split('-')[0]  ## H1, H3-red\n",
    "            self.occupant_names.append(occupant_name)\n",
    "            ishome = []\n",
    "            with open(os.path.join(self.ground_path, occ)) as csv_file:\n",
    "                csv_reader, line_count = csv.reader(csv_file, delimiter=','), 0\n",
    "                for row in csv_reader:\n",
    "                    status, when = row[1], row[2].split('at')\n",
    "                    dt_day = datetime.strptime(str(when[0] + when[1]), '%B %d, %Y  %I:%M%p')\n",
    "                    ishome.append((status, dt_day))\n",
    "                    if line_count == 0:\n",
    "                        enter_times.append(dt_day)\n",
    "                    line_count += 1\n",
    "                exit_times.append(dt_day)\n",
    "                \n",
    "            occupants[occupant_name] = ishome        \n",
    "        self.first_last = (sorted(enter_times)[0], sorted(exit_times)[-1])\n",
    "        return occupants\n",
    "    \n",
    "    def create_occupancy_df(self, occupants, frequency):\n",
    "        occ_range = pd.date_range(start=self.first_last[0], end=self.first_last[1], freq=frequency)    \n",
    "        occ_df = pd.DataFrame(index=occ_range)\n",
    "        \n",
    "        for occ in occupants:\n",
    "            occ_df[occ] = 99\n",
    "            s1 = 'exited'\n",
    "            for r in occupants[occ]:\n",
    "                date = r[1]\n",
    "                s2 = r[0]                \n",
    "                occ_df.loc[(occ_df.index < date) & (occ_df[occ]==99) & (s1 == 'exited') & (s2 == 'entered'), occ] =  0\n",
    "                occ_df.loc[(occ_df.index < date) & (occ_df[occ]==99) & (s1 == 'entered') & (s2 == 'exited'), occ] =  1\n",
    "                s1 = s2               \n",
    "            occ_df.loc[(occ_df.index >= date) & (occ_df[occ] == 99) & (s1 == 'entered'), occ] = 1\n",
    "            occ_df.loc[(occ_df.index >= date) & (occ_df[occ] == 99) & (s1 == 'exited'), occ] = 0    \n",
    "            \n",
    "        occ_df['number'] = occ_df[list(occupants.keys())].sum(axis = 1)\n",
    "        occ_df['occupied'] = 0\n",
    "        occ_df.loc[occ_df['number'] > 0, 'occupied'] = 1\n",
    "        return (occ_df)\n",
    "    \n",
    "    \n",
    "    def average_df(self, df):\n",
    "        time_series = []\n",
    "        for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "            df_max = df_chunk.max()\n",
    "            df_index = df_chunk.iloc[-1]\n",
    "            time_series.append(df_index.name)\n",
    "            df_summary = df_max.to_frame().transpose() \n",
    "            new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "\n",
    "        new_df.index = time_series  \n",
    "        new_df = new_df[['number', 'occupied']]\n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def not_average_df(self, df):\n",
    "#         time_series = []\n",
    "#         for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "#             df_max = df_chunk.max()\n",
    "#             df_index = df_chunk.iloc[-1]\n",
    "#             time_series.append(df_index.name)\n",
    "#             df_summary = df_max.to_frame().transpose() \n",
    "#             new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "\n",
    "#         new_df.index = time_series  \n",
    "#         new_df = new_df[['number', 'occupied']]\n",
    "#         return new_df\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    def write_occupancy_csv(self, df, fname):       \n",
    "        fname = os.path.join(self.write_dir, fname)\n",
    "        if not os.path.isfile(fname):\n",
    "            df.to_csv(fname, index = True)\n",
    "            print(fname + ': Write Sucessful!')\n",
    "        else:\n",
    "            print(fname + ': File already exists')    \n",
    "\n",
    "            \n",
    "    def main(self):\n",
    "        occupant_status = self.get_ground_truth()\n",
    "        df = self.create_occupancy_df(occupant_status, frequency=self.occ_freq)\n",
    "        self.df = df[['number', 'occupied']]\n",
    "    \n",
    "\n",
    "        #self.df = self.average_df(df)        \n",
    "        #create larger grain for viewing (1 minute frequency)\n",
    "        #write_df = self.create_occupancy_df(occupant_status, frequency='60S')\n",
    "        #self.write_occupancy_csv(write_df, '{}-{}-Occupancy_df.csv'.format(self.home, self.system))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadEnv(HomeData):\n",
    "    \n",
    "    def __init__(self, path, sensor_hub):\n",
    "        HomeData.__init__(self, path)\n",
    "        self.name = sensor_hub\n",
    "        self.env_dir = os.path.join(self.root_dir, self.name, 'env_params')\n",
    "        self.num_folders = 288\n",
    "        self.files_per = 5\n",
    "        self.minutes_per_day = 1440\n",
    "        self.all_data = {}\n",
    "        self.first_last = {}\n",
    "        self.total_minutes = {}\n",
    "        self.details = []\n",
    "\n",
    "        \n",
    "    def get_date_folders(self, path):\n",
    "        date_folders = self.mylistdir(path)\n",
    "        date_folders.sort()\n",
    "        self.day1, self.dayn = date_folders[0], date_folders[-1]\n",
    "        return date_folders\n",
    "\n",
    "    def read_in_data(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            try:\n",
    "                self.data_dicts = json.loads(f.read())\n",
    "                for time_point in self.data_dicts:\n",
    "                    for measure in time_point:\n",
    "                        self.measurements[measure].append(time_point[measure])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    \n",
    "    def get_all_data(self, path, day):\n",
    "        self.measurements = {\n",
    "            'time':[], 'tvoc_ppb':[], 'temp_c':[], 'rh_percent':[], \n",
    "            'light_lux':[],'co2eq_ppm':[], 'dist_mm':[], 'co2eq_base':[], 'tvoc_base':[]}\n",
    "        file_path = os.path.join(path, day)\n",
    "        minute_folders = self.mylistdir(file_path)\n",
    "        minute_folders.sort()\n",
    "        num_missing = 5 * (self.num_folders - len(minute_folders))\n",
    "        min_1, min_L = minute_folders[0], minute_folders[-1]\n",
    "        min_n = str(int(min_L) + 4).zfill(4)\n",
    "        self.first_last[day] = min_1, min_n\n",
    "        for minute in minute_folders:\n",
    "            sub_files_path = os.path.join(file_path, minute)\n",
    "            sub_files = self.mylistdir(sub_files_path)\n",
    "            sub_files.sort()\n",
    "            missing = self.files_per - len(sub_files)\n",
    "            num_missing += missing\n",
    "            for file in sub_files:\n",
    "                if file.endswith('.json'):\n",
    "                    self.read_in_data(os.path.join(sub_files_path, file))\n",
    "        \n",
    "        self.all_data[day] = self.measurements\n",
    "        total_day = 1440 - num_missing\n",
    "        self.total_minutes[day] = total_day\n",
    "        \n",
    "    \n",
    "    def get_day_summary(self, day):\n",
    "        self.get_all_data(self.env_dir, day)\n",
    "        try:\n",
    "            total = self.total_minutes[day]/self.minutes_per_day\n",
    "            perc = '{:.2f}'.format(total)\n",
    "        except Exception as e:\n",
    "            print('except: {}'.format(e))\n",
    "            perc = 0.00\n",
    "        F1, F2 = self.first_last[day][0], self.first_last[day][1]\n",
    "        s = (f'({F1[0:2]}:{F1[2:4]}, {F2[0:2]}:{F2[2:4]})')\n",
    "        details = '{} {} {} {}'.format(self.name, day, s, perc)\n",
    "        return details, total\n",
    "\n",
    "      \n",
    "    def get_date_splits(self, dates):\n",
    "        dt_dates = [datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "        date_lists = self.date_segments(dt_dates)\n",
    "        all_lists = [[date.strftime('%Y-%m-%d') for date in sublist] for sublist in date_lists]\n",
    "        return all_lists\n",
    "    \n",
    "    \n",
    "    def read_all_days(self):\n",
    "        dates_to_use = []\n",
    "        date_folders = self.get_date_folders(self.env_dir)\n",
    "        fname = os.path.join(self.write_dir, '{}-{}-data-summary.txt'.format(self.home, self.name))\n",
    "        with open(fname, 'w+') as writer:\n",
    "            for day in date_folders:\n",
    "                day_details, total = self.get_day_summary(day)\n",
    "                #print(day_details)\n",
    "                writer.write(day_details + '\\n')\n",
    "                self.details.append(day_details)\n",
    "                if total > 0.85:\n",
    "                    dates_to_use.append(day)               \n",
    "            self.lists_of_dates = self.get_date_splits(dates_to_use)    \n",
    "        writer.close()\n",
    "        print(fname + ': Write Sucessful!')\n",
    "    \n",
    "   \n",
    "    def main(self):\n",
    "        self.read_all_days()\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanEnvData(HomeData):\n",
    "    \n",
    "    def __init__(self, path, lists, data, hubs, occ):\n",
    "        HomeData.__init__(self, path)\n",
    "        self.all_data = data\n",
    "        self.dates_in_common = lists\n",
    "        self.sensor_hubs = hubs\n",
    "        self.occupancy_df = occ\n",
    "        self.all_dfs = {}\n",
    "        self.var_names1 = ['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm', 'abs_humid']\n",
    "        self.var_names2 = ['home', 'sensor']\n",
    "        \n",
    "    def csv_name(self, name, day):       \n",
    "        return str(self.home + '_' + name + '_' + day + '.csv')     \n",
    "        \n",
    "    def create_full_dfs(self, df, D1):\n",
    "        df_fullday = self.make_date_range(day1 = D1) ##use this for full 24hours\n",
    "        df2 = df.reindex(df_fullday, fill_value = np.nan) \n",
    "        df2.fillna(np.nan)\n",
    "        return df2\n",
    "        \n",
    "        \n",
    "    def make_date_range(self, day1, dayn=None, t1 = '0000', tn = '2359'):\n",
    "        self.range_start = str(day1 + ' ' + t1[0:2] + ':' + t1[2:4] + ':00')\n",
    "        self.range_end = str(day1 + ' ' + tn[0:2] + ':' + tn[2:4] + ':50')\n",
    "        date_range = pd.date_range(start=self.range_start, end=self.range_end, freq='10s')\n",
    "        return date_range \n",
    "        \n",
    "        \n",
    "    def clean_dates(self, name, day, df): \n",
    "        df['time'] = df['time'].str.strip('Z').str.replace('T',' ')\n",
    "        df['datetime_index'] = pd.to_datetime(df['time'])         \n",
    "        df = df.set_index('datetime_index')\n",
    "        df.index = df.index.floor('10s')\n",
    "        df2 = self.create_full_dfs(df, day)        \n",
    "        str_date = df2.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df2.insert(loc = 0, column = 'str_datetime', value = str_date)\n",
    "        datetime_col = df2['str_datetime'].str.split(' ', n = 1, expand = True)         \n",
    "        df2.insert(loc = 0, column = 'date', value = datetime_col[0])\n",
    "        df2.insert(loc = 0, column = 'time-hr-min-sec', value = datetime_col[1])\n",
    "        time_col = datetime_col[1].str.split(':', n = 2, expand = True)    \n",
    "        df2.insert(loc = 0, column = 'second', value = time_col[2])\n",
    "        df2.insert(loc = 0, column = 'minute', value = time_col[1])\n",
    "        df2.insert(loc = 0, column = 'hour', value = time_col[0])        \n",
    "        df2 = df2.drop(columns = ['str_datetime', 'time'])\n",
    "        df2 = df2.sort_values(by = ['date', 'hour', 'minute', 'second'])\n",
    "        df2['home'] = self.home\n",
    "        df2['sensor'] = name\n",
    "        #df2 = df2.drop(columns = ['hour', 'minute', 'second', 'time-hr-min-sec', 'date'])\n",
    "\n",
    "        return df2     \n",
    "    \n",
    "    \n",
    "    def absolute_humidity(self, df):\n",
    "        df['abs_humid'] = 13.247*df['rh_percent']*(2.718281828459045**((17.67*df['temp_c'])/(243.5+df['temp_c']))/(273.15+df['temp_c']))\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def check_rh(self, df, day, limit=3000):\n",
    "        big_rh = df.loc[df.rh_percent > limit]\n",
    "#         if len(big_rh) > 0:\n",
    "#             print(big_rh)\n",
    "#         else:\n",
    "#             print('No high value rh for day {}'.format(day))\n",
    "        df.loc[df.rh_percent > limit, 'rh_percent'] = np.nan\n",
    "        return df \n",
    "    \n",
    "    \n",
    "    def make_single_dfs(self):\n",
    "        for sensor_hub in self.sensor_hubs:\n",
    "            for date_list in self.dates_in_common:\n",
    "                #day1, dayn = date_list[0], date_list[-1]\n",
    "                for day in date_list:\n",
    "                    new_df = pd.DataFrame.from_dict(self.all_data[sensor_hub][day])\n",
    "                    new_df = self.absolute_humidity(new_df)\n",
    "                    new_df = self.check_rh(new_df, day)  \n",
    "                    clean_df = self.clean_dates(sensor_hub, day, new_df)\n",
    "                    self.write_data(sensor_hub, day, clean_df)\n",
    "    \n",
    "    \n",
    "    def write_data(self, hub, df_to_write, folder, title):\n",
    "        storage_path = self.make_storage_directory(os.path.join(self.root_dir, hub, folder))\n",
    "        target_fname = os.path.join(storage_path, self.csv_name(hub, title)) \n",
    "        if not os.path.isfile(target_fname):\n",
    "            df_to_write.to_csv(target_fname, index_label = 'timestamp', index = True)\n",
    "            print(target_fname + ': Write Sucessful!')\n",
    "        else:\n",
    "            print(target_fname + ': File already exists')    \n",
    "            \n",
    "    \n",
    "    def join_dfs(self, date_list, sensor_hub):\n",
    "        df_list = []\n",
    "        for day in date_list:\n",
    "            new_df = pd.DataFrame.from_dict(self.all_data[sensor_hub][day])\n",
    "            new_df = self.absolute_humidity(new_df)\n",
    "            new_df = self.check_rh(new_df, day)  \n",
    "            clean_df = self.clean_dates(sensor_hub, day, new_df)\n",
    "            self.write_data(hub=sensor_hub, df_to_write=clean_df, folder='Complete_CSV', title = day)\n",
    "            df_list.append(clean_df)\n",
    "            \n",
    "        full_df = pd.concat(df_list)\n",
    "        full_df = full_df.ffill(limit = 30)\n",
    "        full_df = full_df.bfill(limit = 30) \n",
    "        full_df = full_df.drop(columns = ['hour', 'minute', 'second', 'time-hr-min-sec', 'date'])\n",
    "        \n",
    "        return full_df\n",
    "    \n",
    "    \n",
    "    def average_dfs(self, df):\n",
    "        time_series = []\n",
    "        \n",
    "        for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "            df_means = df_chunk[self.var_names1].mean()\n",
    "            df_index = df_chunk.iloc[-1][self.var_names2]\n",
    "            time_series.append(df_index.name)\n",
    "            sr_summary = df_index.append(df_means, ignore_index = False)\n",
    "            df_summary = sr_summary.to_frame().transpose()   \n",
    "            new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "        \n",
    "        new_df.index = time_series\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "     \n",
    "    def attach_occupancy(self, df):\n",
    "        joined_df = pd.concat([df, self.occupancy_df], axis=1, join='inner')\n",
    "        return joined_df\n",
    "\n",
    "        \n",
    "        \n",
    "    def main(self): \n",
    "        for sensor_hub in self.sensor_hubs:\n",
    "            sensor_full_dfs = []\n",
    "            for date_list in self.dates_in_common:\n",
    "                day1, dayn = date_list[0], date_list[-1]\n",
    "                \n",
    "                full_df = self.join_dfs(date_list, sensor_hub)\n",
    "#                 self.write_data(hub=sensor_hub, df_to_write=full_df, folder='Stacked_CSV', title='{}_to_{}'.format(day1, dayn)) \n",
    "                \n",
    "                full_with_occ = self.attach_occupancy(full_df)\n",
    "                full_with_occ = full_with_occ.drop(columns = ['co2eq_base', 'tvoc_base'])\n",
    "                self.write_data(hub=sensor_hub, df_to_write=full_with_occ, folder='Stacked_CSV_10sec', title='{}_to_{}-occ'.format(day1, dayn)) \n",
    "#                 print(full_with_occ.columns)\n",
    "\n",
    "#                 averaged_df = self.average_dfs(full_df)\n",
    "#                 cols = averaged_df.columns.tolist()\n",
    "#                 cols = cols[2:] + cols[:2]\n",
    "#                 averaged_df = averaged_df[cols]               \n",
    "#                 df_w_occ = self.attach_occupancy(averaged_df)\n",
    "#                 self.write_data(hub=sensor_hub, df_to_write=df_w_occ, folder='Stacked_CSV_5min', title='{}_to_{}-averaged-occ'.format(day1, dayn))    \n",
    "\n",
    "                \n",
    "                #sensor_full_dfs.append(df_w_occ)               \n",
    "            self.all_dfs[sensor_hub] = sensor_full_dfs\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red'\n",
    "sensors_red = ['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n",
    "sensors_black = ['BS1', 'BS2', 'BS3', 'BS4', 'BS5', 'BS6']\n",
    "\n",
    "\n",
    "R_set = [('r{}'.format(x), 'RS{}'.format(x)) for x in np.arange(1,6)]\n",
    "B_set = [('b{}'.format(x), 'BS{}'.format(x)) for x in np.arange(1,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS1-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS2-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS3-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS4-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS5-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/Summaries/H3-BS6-data-summary.txt: Write Sucessful!\n",
      "\n",
      "\n",
      "*** There are 3 lists ***\n",
      "\n",
      "['2019-04-18', '2019-04-19', '2019-04-20', '2019-04-21', '2019-04-22', '2019-04-23', '2019-04-24', '2019-04-25', '2019-04-26']\n",
      "['2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09']\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21', '2019-05-22', '2019-05-23', '2019-05-24', '2019-05-25']\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Stacked_CSV_10sec/H3_BS1_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Stacked_CSV_10sec/H3_BS1_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Complete_CSV/H3_BS1_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS1/Stacked_CSV_10sec/H3_BS1_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Stacked_CSV_10sec/H3_BS2_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Stacked_CSV_10sec/H3_BS2_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Complete_CSV/H3_BS2_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS2/Stacked_CSV_10sec/H3_BS2_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Stacked_CSV_10sec/H3_BS3_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Stacked_CSV_10sec/H3_BS3_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Complete_CSV/H3_BS3_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS3/Stacked_CSV_10sec/H3_BS3_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Stacked_CSV_10sec/H3_BS4_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Stacked_CSV_10sec/H3_BS4_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Complete_CSV/H3_BS4_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS4/Stacked_CSV_10sec/H3_BS4_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Stacked_CSV_10sec/H3_BS5_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Stacked_CSV_10sec/H3_BS5_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Complete_CSV/H3_BS5_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS5/Stacked_CSV_10sec/H3_BS5_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-04-26.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Stacked_CSV_10sec/H3_BS6_2019-04-18_to_2019-04-26-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-04.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-05.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-06.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Stacked_CSV_10sec/H3_BS6_2019-05-04_to_2019-05-09-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-17.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-18.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-22.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-23.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-24.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Complete_CSV/H3_BS6_2019-05-25.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H3/H3-black/BS6/Stacked_CSV_10sec/H3_BS6_2019-05-17_to_2019-05-25-occ.csv: Write Sucessful!\n",
      "Index(['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm',\n",
      "       'abs_humid', 'home', 'sensor', 'number', 'occupied'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "o = HomeOccupancy(path)\n",
    "o.main()\n",
    "\n",
    "all_sensor_data = {}\n",
    "all_dates_to_use = {}\n",
    "all_details = {}\n",
    "\n",
    "for sensor in sensors_black:\n",
    "    s = ReadEnv(path, sensor)\n",
    "    s.main()\n",
    "    all_sensor_data[sensor] = s.all_data\n",
    "    all_dates_to_use[sensor] = s.lists_of_dates\n",
    "#     all_details[sensor] = s.details\n",
    "\n",
    "# Lists = [(tuple(r1), tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n",
    "#          for r1 in all_dates_to_use['RS1']\n",
    "#          for r2 in all_dates_to_use['RS2']\n",
    "#          for r3 in all_dates_to_use['RS3']\n",
    "#          for r4 in all_dates_to_use['RS4']\n",
    "#          for r5 in all_dates_to_use['RS5']]   \n",
    "\n",
    "Lists = [(tuple(b1), tuple(b2), tuple(b3), tuple(b4), tuple(b5), tuple(b6)) \n",
    "         for b1 in all_dates_to_use['BS1']\n",
    "         for b2 in all_dates_to_use['BS2']\n",
    "         for b3 in all_dates_to_use['BS3']\n",
    "         for b4 in all_dates_to_use['BS4']\n",
    "         for b5 in all_dates_to_use['BS5']\n",
    "         for b6 in all_dates_to_use['BS6']]    \n",
    "  \n",
    "        \n",
    "same_dates = []\n",
    "for L in Lists:\n",
    "    same_lists = set(L[0]).intersection(*L)\n",
    "    if len(same_lists) > 0:\n",
    "        same_dates.append(sorted(same_lists))\n",
    "\n",
    "print('\\n\\n*** There are {} lists ***\\n'.format(len(same_dates)))\n",
    "for l in same_dates:\n",
    "    print(l) \n",
    "\n",
    "e = CleanEnvData(path, same_dates, all_sensor_data, sensors_black, o.df)\n",
    "e.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = HomeOccupancy(path)\n",
    "o.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/Summaries/H5-RS1-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/Summaries/H5-RS2-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/Summaries/H5-RS3-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/Summaries/H5-RS4-data-summary.txt: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/Summaries/H5-RS5-data-summary.txt: Write Sucessful!\n"
     ]
    }
   ],
   "source": [
    "all_sensor_data = {}\n",
    "all_dates_to_use = {}\n",
    "all_details = {}\n",
    "\n",
    "for sensor in sensors_red:\n",
    "    s = ReadEnv(path, sensor)\n",
    "    s.main()\n",
    "    all_sensor_data[sensor] = s.all_data\n",
    "    all_dates_to_use[sensor] = s.lists_of_dates\n",
    "#     all_details[sensor] = s.details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensor RS1 list 1 has 2 dates\n",
      "['2019-06-04', '2019-06-05']\n",
      "\n",
      "Sensor RS1 list 2 has 12 dates\n",
      "['2019-06-19', '2019-06-20', '2019-06-21', '2019-06-22', '2019-06-23', '2019-06-24', '2019-06-25', '2019-06-26', '2019-06-27', '2019-06-28', '2019-06-29', '2019-06-30']\n",
      "\n",
      "Sensor RS2 list 1 has 27 dates\n",
      "['2019-06-04', '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16', '2019-06-17', '2019-06-18', '2019-06-19', '2019-06-20', '2019-06-21', '2019-06-22', '2019-06-23', '2019-06-24', '2019-06-25', '2019-06-26', '2019-06-27', '2019-06-28', '2019-06-29', '2019-06-30']\n",
      "\n",
      "Sensor RS3 list 1 has 18 dates\n",
      "['2019-06-04', '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16', '2019-06-17', '2019-06-18', '2019-06-19', '2019-06-20', '2019-06-21']\n",
      "\n",
      "Sensor RS3 list 2 has 6 dates\n",
      "['2019-06-25', '2019-06-26', '2019-06-27', '2019-06-28', '2019-06-29', '2019-06-30']\n",
      "\n",
      "Sensor RS4 list 1 has 13 dates\n",
      "['2019-06-04', '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16']\n",
      "\n",
      "Sensor RS4 list 2 has 3 dates\n",
      "['2019-06-19', '2019-06-20', '2019-06-21']\n",
      "\n",
      "Sensor RS5 list 1 has 15 dates\n",
      "['2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16', '2019-06-17', '2019-06-18', '2019-06-19', '2019-06-20', '2019-06-21']\n",
      "\n",
      "Sensor RS5 list 2 has 1 dates\n",
      "['2019-06-25']\n",
      "\n",
      "\n",
      "*** There are 2 lists ***\n",
      "\n",
      "['2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16']\n",
      "['2019-06-19', '2019-06-20', '2019-06-21']\n"
     ]
    }
   ],
   "source": [
    "# Lists = [(tuple(r1), tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n",
    "#          for r1 in all_dates_to_use['RS1']\n",
    "#          for r2 in all_dates_to_use['RS2']\n",
    "#          for r3 in all_dates_to_use['RS3']\n",
    "#          for r4 in all_dates_to_use['RS4']\n",
    "#          for r5 in all_dates_to_use['RS5']]   \n",
    "  \n",
    "    \n",
    "    \n",
    "Lists = [(tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n",
    "         for r2 in all_dates_to_use['RS2']\n",
    "         for r3 in all_dates_to_use['RS3']\n",
    "         for r4 in all_dates_to_use['RS4']\n",
    "         for r5 in all_dates_to_use['RS5']]   \n",
    "\n",
    "# Lists = [(tuple(b1), tuple(b2), tuple(b3), tuple(b4), tuple(b5), tuple(b6)) \n",
    "#          for b1 in all_dates_to_use['BS1']\n",
    "#          for b2 in all_dates_to_use['BS2']\n",
    "#          for b3 in all_dates_to_use['BS3']\n",
    "#          for b4 in all_dates_to_use['BS4']\n",
    "#          for b5 in all_dates_to_use['BS5']\n",
    "#          for b6 in all_dates_to_use['BS6']]    \n",
    "\n",
    "\n",
    "for s in all_dates_to_use:\n",
    "    for i, l in enumerate(all_dates_to_use[s]):\n",
    "        print('\\nSensor {} list {} has {} dates'.format(s, i+1, len(l)))\n",
    "        print(l)    \n",
    "        \n",
    "same_dates = []\n",
    "for L in Lists:\n",
    "    same_lists = set(L[0]).intersection(*L)\n",
    "    if len(same_lists) > 0:\n",
    "        same_dates.append(sorted(same_lists))\n",
    "\n",
    "print('\\n\\n*** There are {} lists ***\\n'.format(len(same_dates)))\n",
    "for l in same_dates:\n",
    "    print(l) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n",
      "['RS2', 'RS3', 'RS4', 'RS5']\n",
      "[['2019-06-19', '2019-06-20', '2019-06-21']]\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-10.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-11.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-12.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-13.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-14.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-15.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-16.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Stacked_CSV_10sec/H5_RS2_2019-06-07_to_2019-06-16-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Complete_CSV/H5_RS2_2019-06-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS2/Stacked_CSV_10sec/H5_RS2_2019-06-19_to_2019-06-21-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-10.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-11.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-12.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-13.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-14.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-15.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-16.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Stacked_CSV_10sec/H5_RS3_2019-06-07_to_2019-06-16-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Complete_CSV/H5_RS3_2019-06-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS3/Stacked_CSV_10sec/H5_RS3_2019-06-19_to_2019-06-21-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-10.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-11.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-12.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-13.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-14.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-15.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-16.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Stacked_CSV_10sec/H5_RS4_2019-06-07_to_2019-06-16-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Complete_CSV/H5_RS4_2019-06-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS4/Stacked_CSV_10sec/H5_RS4_2019-06-19_to_2019-06-21-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-07.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-08.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-09.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-10.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-11.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-12.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-13.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-14.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-15.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-16.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Stacked_CSV_10sec/H5_RS5_2019-06-07_to_2019-06-16-occ.csv: Write Sucessful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-19.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-20.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Complete_CSV/H5_RS5_2019-06-21.csv: File already exists\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H5/H5-red/RS5/Stacked_CSV_10sec/H5_RS5_2019-06-19_to_2019-06-21-occ.csv: Write Sucessful!\n"
     ]
    }
   ],
   "source": [
    "dates = [same_dates[1]]\n",
    "sensors = sensors_red[1:]\n",
    "print(sensors_red)\n",
    "print(sensors)\n",
    "print(dates)\n",
    "\n",
    "e = CleanEnvData(path, same_dates, all_sensor_data, sensors, o.df)\n",
    "\n",
    "\n",
    "#e = CleanEnvData(path, same_dates, all_sensor_data, sensors_black, o.df)\n",
    "\n",
    "e.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n",
      "['RS2', 'RS3', 'RS4', 'RS5']\n"
     ]
    }
   ],
   "source": [
    "sensors = sensors_red[1:]\n",
    "print(sensors_red)\n",
    "print(sensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
